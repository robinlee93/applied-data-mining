{
    "contents" : "\n\n\\title{HW5 For Applied Data Mining \\\\ STAT W 3026-4026 \\\\ Spring 2016 \\\\ Columbia University}\n\\author{\n       Robin Lee  \\\\\n               rcl2136\\\\\n        QMSS MA}\n\\date{\\today}\n\n\\documentclass[12pt]{article}\n\n\\makeatletter\n\\newcommand*{\\rom}[1]{\\expandafter\\@slowromancap\\romannumeral #1@}\n\\makeatother\n\\usepackage{amsthm,amsmath,graphicx,csquotes}\n\\usepackage[\n         colorlinks=true,\n         linkcolor=black,\n         citecolor=black,\n         urlcolor=black]\n         {hyperref}  \n         \n\\newtheorem{thm}{Theorem}\n\\newtheorem{defn}{Definition}\n%\\newtheorem{rem}{Remark}\n%\\newtheorem{lem}{Lemma}\n\\newtheorem{prop}{Property}\n\n\n\\begin{document}\n\\SweaveOpts{concordance=TRUE}\n\\maketitle\n\n\n\\section{Instruction}\nFor the Boston Housing Data, firstpartition the data as training (2/3) and testing (1/3).\nThen fit each the following models in table below (Linear Reg, Lasso, ElasticNet, PLS, Neural Networks, MARS, SVM, K-NN) and present the performance\nmeasures for both training and test data. Use cross validation to tune the parameters. \n\n\\newpage\n\n\\section{Result}\n\\begin{table}[ht]\n\n\\centering % used for centering table\n\n\\begin{tabular}{c c c c c} % centered columns (4 columns)\n\n\\hline %inserts horizontal lines\n\n& & Training & Testing \\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\n& RMSE & R2 & RMSE & R2 \\\\ [0.5ex] % inserts table\n\n\\hline % inserts single horizontal line\n\nLinear Regression & 4.515 & 0.775 & 5.587  & 0.614 \\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\nLasso & 4.528 & 0.769 & 5.613 & 0.609\\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\nElasticNet &  4.531 & 0.776 &  5.586 & 0.613\\\\ [0.5ex] % inserts table\n\\hline %inserts horizontal lines\n\nPLS & 7.434 & 0.374 & 7.704 & 0.272\\\\ [0.5ex] % inserts table\n\\hline %inserts horizontal lines\n\nNeural Networks & 23.578 &  NA &  23.019 & NA \\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\nMARS & 3.108 &  0.888 & 4.718 & 0.735\\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\nSVM & 3.667 & 0.855 & 4.566 & 0.744\\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\nK-NN &  6.369 & 0.547 & 6.749 & 0.446\\\\ [0.5ex] % inserts table\n\n\\hline %inserts horizontal lines\n\n\\end{tabular}\n\n\\label{table:nonlin} % is used to refer this table in the text\n\n\\end{table}\n\n\n\n\n\n\n\\section{Step 1 - Create Data Partition}\n\n<<>>=\nlibrary(caret)\nlibrary(MASS)\ndata(\"Boston\")\n\nset.seed(569)\ntrain_index <- createDataPartition(Boston$medv, p = 2/3, \n                                  list = FALSE, times = 1)\ntrain <- Boston[train_index, ] \ntest <- Boston[-train_index, ]\n@\n\n\\section{Set CV}\n<<>>=\ncontrol <- trainControl( # 10 fold CV, repeated 10 times\n  method = 'repeatedcv', number = 10, repeats = 10\n  \n)\n@\n\n\\section{Linear Regression}\n<<eval = FALSE>>=\nlm1 <- train(medv ~ . , data = train, \n            method = \"lm\", \n            trControl = control)\nlm1\n\nlm_test <- predict(lm1, test)\npostResample(lm_test, test$medv)\n@\n\n\\section{Lasso}\n<<eval = FALSE>>=\nlasso1 <- train(medv ~ . , data = train, \n            method = \"lasso\", \n            trControl = control)\nlasso1\n\nlasso_test <- predict(lasso1, test)\npostResample(lasso_test, test$medv)\n@\n\n\\section{Elastic Net}\n<<eval = FALSE>>=\nenet1 <- train(medv ~ . , data = train, \n            method = \"enet\", \n            trControl = control)\nenet1\n\nenet_test <- predict(enet1, test)\npostResample(enet_test, test$medv)\n@\n\n\\section{Partial Least Square}\n<<eval = FALSE>>=\npls1 <- train(medv ~ . , data = train, \n            method = \"pls\", \n            trControl = control)\npls1\n\npls_test <- predict(pls1, test)\npostResample(pls_test, test$medv)\n@\n\n\\section{Neural Net}\n<<eval = FALSE>>=\nnnet1 <- train(medv ~ . , data = train, \n            method = \"nnet\", \n            trControl = control)\nnnet1\n\nnnet_test <- predict(nnet1, test)\npostResample(nnet_test, test$medv)\n@\n\n\n\\section{MARS}\n<<>>=\nmars1 <- train(medv ~ . , data = train, \n            method = \"earth\", \n            trControl = control)\nmars1\n\nmars_test <- predict(mars1, test)\npostResample(mars_test, test$medv)\n@\n\n\\section{SVM}\n<<eval = FALSE>>=\nsvm1 <- train(medv ~ . , data = train, \n            method = \"svmRadial\", \n            trControl = control)\nsvm1\n\nsvm_test <- predict(svm1, test)\npostResample(svm_test, test$medv)\n@\n\n\\section{K Nearest Neighbor}\n<<eval = FALSE>>=\nknn1 <- train(medv ~ . , data = train, \n            method = \"knn\", \n            trControl = control)\nknn1\n\nknn_test <- predict(knn1, test)\npostResample(knn_test, test$medv)\n@\n\n\\section{Question 2}\nThe lm.ridge function would not fit age twice. I created an age2 variable that is the same as age. Fitting it twice age with lm.ridge does not give the expected coefficient . It is not one half of the original coefficient. \n<<eval=TRUE>>=\n\nridgem <- lm(medv ~ age , data = train)\nridgem\n\ntrain_add <- train\ntrain_add$age2 <- train_add$age\nridgem2 <- lm.ridge(medv ~ age + age2, data = train_add)\nridgem2$coef\n\n\n\n@\n\\end{document}\n",
    "created" : 1458746205843.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1595959901",
    "id" : "C6065DB1",
    "lastKnownWriteTime" : 1458823517,
    "path" : "~/stat-apm/Lee_Robin_hw5.Rnw",
    "project_path" : "Lee_Robin_hw5.Rnw",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "sweave"
}